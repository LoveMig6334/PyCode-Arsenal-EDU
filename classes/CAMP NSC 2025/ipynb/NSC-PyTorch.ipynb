{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc0ebe6-5fe2-495e-bc50-9bec4e7840ae",
   "metadata": {},
   "source": [
    "# Image Classification (Pokemon Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31bf3f-f26e-4922-b408-ed312a43b81e",
   "metadata": {},
   "source": [
    "### Install necessary components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e830d-3539-47e5-a2bf-0484d204af1e",
   "metadata": {},
   "source": [
    "### Import all model training modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5817ac4-6f33-47c7-a8b9-1528d9cc14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23050f61-a04b-496a-b7b9-5f9e42b668ba",
   "metadata": {},
   "source": [
    "### Set a seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0226d0-d554-4ce0-8bb4-93033147b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(f\"Set a random seed to: {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a1a99-6873-4ea9-8eaa-2a7693486ec3",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a53fc3-1e61-4a08-9b51-5f7b4b55f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(f\"default Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5932ce-4dfd-4725-a302-312695926299",
   "metadata": {},
   "source": [
    "### Define dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b346c2e-55d3-4274-b764-239ce60a5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581857e-bdff-4611-b769-2670c27ee3c8",
   "metadata": {},
   "source": [
    "### Define transformations (resize and normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ca18f-d4ac-4662-b8de-182fae13b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            (image_size, image_size)\n",
    "        ),  # Resize images to image_size x image_size\n",
    "        transforms.ToTensor(),  # Convert to Tensor\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalize images\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef149b7-97e0-415e-b2b3-f0a2a0cb3976",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd556d-2927-4d5e-9c22-d3a20cfdc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e2374-1b0d-4396-a02a-31bdf8f69c70",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a3e54-c92b-4567-b009-302c6db04542",
   "metadata": {},
   "source": [
    "### Import all virtualizing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5385fb5-1972-4cf4-869e-35dc0cd9ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Button, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d7430-da7f-4ea9-9db3-a6cd0a6f9563",
   "metadata": {},
   "source": [
    "### Function to display the image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f745db-dbba-45ad-8a90-48286eab5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(index):\n",
    "    # Get the image and label\n",
    "    image, label = dataset[index]\n",
    "\n",
    "    # Convert the image from tensor to numpy format for displaying\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Clear the output display and plot\n",
    "    output.clear_output(wait=True)  # type: ignore\n",
    "    with output:  # type: ignore\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Label: {dataset.classes[label]}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd8d53-f8b6-4137-bff6-f62a0f562bda",
   "metadata": {},
   "source": [
    "### Button click event to display the next image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4daf7-3218-4f3b-adf9-91a04cff57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfb11c-652d-4e57-8094-7e17f609dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_button_clicked(b):\n",
    "    global index\n",
    "    index = (index + random.randint(1, len(dataset) - 1)) % len(\n",
    "        dataset\n",
    "    )  # Move to the next image, loop back at the end\n",
    "    show_image(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be15a5-3a87-4a05-b19a-c6d10fdd34cc",
   "metadata": {},
   "source": [
    "### Initialize output display for Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de118f41-24f4-44b8-8a2d-8bc57136277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Output()\n",
    "next_button = Button(description=\"Next Image\")\n",
    "next_button.on_click(on_button_clicked)\n",
    "display(next_button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a514447-c480-45a7-819e-88dc875abb16",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab18db4-5561-45f6-bcaa-7c0b089e1910",
   "metadata": {},
   "source": [
    "### Split dataset into train and test sets (80-20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a595e1e-ff09-49f4-8c8e-fb1b42081490",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size], generator=torch.Generator().manual_seed(seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33c76a-19e1-404b-800f-389b61f4c613",
   "metadata": {},
   "source": [
    "### Create DataLoaders for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ab83a-df5e-46c0-bd27-2d1350982349",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, generator=g)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93dc325-75c4-469c-8286-bc6b5ceb88d5",
   "metadata": {},
   "source": [
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c62c5-0b1d-45de-88f0-9feef3887b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=image_size * image_size * 3, hidden_size=1000, num_classes=10\n",
    "    ):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the images\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=image_size * image_size * 3, hidden_size=1000, num_classes=10\n",
    "    ):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the images\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.activation3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=image_size * image_size * 3, hidden_size=1000, num_classes=10\n",
    "    ):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.vgg19 = models.vgg19(pretrained=True)\n",
    "        in_features = self.vgg19.classifier[6].in_features\n",
    "        self.vgg19.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vgg19(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afbf34-9e97-4717-9516-ceef0844fd4a",
   "metadata": {},
   "source": [
    "### Get the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a0ddb-95a4-44b5-b76b-fefe4185f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.dataset.classes\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2551a-5532-4e04-a1fa-bfe2535be8f8",
   "metadata": {},
   "source": [
    "### Initialize the model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1967c-de73-4cdb-b19e-73e2f3fd0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNetwork(input_size=image_size * image_size * 3, num_classes=num_classes).to(\n",
    "    device\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7ee78-3600-4e24-94fc-b267f45e74a4",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b5589-3d57-42c1-b834-c9993a308961",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    batch_num = 0\n",
    "    for images, labels in train_loader:\n",
    "        batch_num += 1\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\n",
    "            f\"\\tEpoch [{epoch+1}/{num_epochs}] Batch #{batch_num}, Loss: {loss.item():.4f}\"\n",
    "        )\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de4635-1b85-4acc-aff9-be679cebd372",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ae06d-04dd-4967-a148-56ece1cbecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d2ca2-f13e-47ca-8075-c132d695f055",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fa286-f703-4ea4-bfd2-eec5eb3541bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNetwork(input_size=image_size * image_size * 3, num_classes=num_classes).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c11f7-9234-438c-ba32-0ca3309aa4cc",
   "metadata": {},
   "source": [
    "### Define test function to evaluate accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d0ae4-461b-4e19-aaf8-a57d9c0c3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac9b85-2bcd-48ec-b96e-8625e4e14558",
   "metadata": {},
   "source": [
    "### Run test for evaluating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31383b3e-e6cd-46f6-be4b-dcf05c6716c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa820b26-640b-4da9-8107-513378b54ca4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2454e78-b01e-4d77-a648-ccfdd13690ae",
   "metadata": {},
   "source": [
    "## Inference with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b718fa-d704-4fe7-a4cb-ba66b2c4540c",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f72869-7495-4bee-a2b1-eb5e9d12c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab989d-c2c3-427a-90cb-bb7b26ade14c",
   "metadata": {},
   "source": [
    "### Define the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479a33a-5791-4347-84f4-affa0ae33708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_pytorch(model, image_path, classes, show_result=False):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode and make the prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, axis=1)\n",
    "        predicted_class = classes[predicted.item()]\n",
    "\n",
    "    if show_result:\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Predicted Class: {predicted_class}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae978a7-825c-42ab-98a0-e6266e0fab3a",
   "metadata": {},
   "source": [
    "### Define the manual folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84079d8-1e5c-4b2c-87d8-2b3ead65e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_folder = \"manual\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17082ae0-cb7f-4dcd-88ff-fe6c56679ff9",
   "metadata": {},
   "source": [
    "### Predict each image in the \"manual\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf8093-dc33-46a1-8cc5-6eb155e55c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pytorch(model, show_result=False):\n",
    "    for image_file in os.listdir(manual_folder):\n",
    "        image_path = os.path.join(manual_folder, image_file)\n",
    "        if os.path.isfile(image_path):  # Check if it's a file\n",
    "            predicted_class = predict_image_pytorch(\n",
    "                model, image_path, class_names, show_result\n",
    "            )\n",
    "            if show_result:\n",
    "                print(f\"Image: {image_file} - Predicted Class: {predicted_class}\")\n",
    "\n",
    "\n",
    "inference_pytorch(model, show_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682b4-1e12-4539-b65d-4098c5611753",
   "metadata": {},
   "source": [
    "## Inference with OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3a3d9-1ce1-434b-a779-9e1ace4c51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f64b8e-6476-4fef-ad07-81d19f5b22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.randn(1, 3, image_size, image_size)\n",
    "ov_model = ov.convert_model(\n",
    "    model,\n",
    "    example_input=(example,),\n",
    "    input=[1, 3, image_size, image_size],\n",
    "    share_weights=False,\n",
    ")\n",
    "ov.save_model(ov_model, \"ov_model.xml\")\n",
    "\n",
    "core = ov.Core()\n",
    "compiled_ov_model = core.compile_model(ov_model, \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da8fe1-fd4d-41a7-a4fe-7b60215e5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_openvino(model, image_path, classes, show_result=False):\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.cvtColor(cv2.imread(filename=str(image_path)), code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize to image_size x image_size\n",
    "    input_image = cv2.resize(src=image, dsize=(image_size, image_size))\n",
    "\n",
    "    # Normalize an image\n",
    "    MEAN = 255 * np.array([0.5, 0.5, 0.5])\n",
    "    STD = 255 * np.array([0.5, 0.5, 0.5])\n",
    "    input_image = np.array(input_image)\n",
    "    input_image = input_image.transpose(-1, 0, 1)\n",
    "    input_image = (input_image - MEAN[:, None, None]) / STD[:, None, None]\n",
    "\n",
    "    # Add batch dimension\n",
    "    input_image = input_image.reshape(1, 3, 32, 32)\n",
    "\n",
    "    # Make the prediction\n",
    "    output_layer = model.output(0)\n",
    "    output = model([input_image])[output_layer]\n",
    "    predicted = np.argmax(output, axis=1)\n",
    "    predicted_class = classes[predicted.item()]\n",
    "\n",
    "    if show_result:\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Predicted Class: {predicted_class}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9f5d7-d8aa-45cd-9eaa-4baa5af8ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_openvino(model, show_result=False):\n",
    "    for image_file in os.listdir(manual_folder):\n",
    "        image_path = os.path.join(manual_folder, image_file)\n",
    "        if os.path.isfile(image_path):  # Check if it's a file\n",
    "            predicted_class = predict_image_openvino(\n",
    "                model, image_path, class_names, show_result\n",
    "            )\n",
    "            if show_result:\n",
    "                print(f\"Image: {image_file} - Predicted Class: {predicted_class}\")\n",
    "\n",
    "\n",
    "inference_openvino(compiled_ov_model, show_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9148a3f-19e8-4305-a5ae-d4a4ef772d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def timeit(callback, run=10, **args):\n",
    "    start_time = time.time()\n",
    "    for _ in range(run):\n",
    "        callback(**args)\n",
    "    end_time = time.time()\n",
    "    print(\"Time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2056ffe-4965-48eb-bf2d-04bb0c1253d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(inference_pytorch, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566cd8-7372-4347-ab36-5938e8fb85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(inference_openvino, model=compiled_ov_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fc854-6608-4a6d-ad49-cdd9c24fb088",
   "metadata": {},
   "source": [
    "# Optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7cc14-803d-48fa-9450-d5c413e8f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    return data_item[0]\n",
    "\n",
    "\n",
    "# Creating separate dataloader with batch size = 1\n",
    "# as dataloaders with batches > 1 is not supported yet.\n",
    "quantization_loader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "quantization_dataset = nncf.Dataset(quantization_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d597d77-fa8d-43f4-871d-ee96b522470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = nncf.quantize(model, quantization_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8481d-d230-4870-9577-4bf2c43ff52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_quantized_model = ov.convert_model(\n",
    "    quantized_model, example_input=(example,), input=[1, 3, image_size, image_size]\n",
    ")\n",
    "ov.save_model(ov_quantized_model, \"ov_quantized_model.xml\")\n",
    "\n",
    "core = ov.Core()\n",
    "compiled_ov_quantized_model = core.compile_model(ov_quantized_model, \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1db021-71cb-4958-8fcf-73a4007b8164",
   "metadata": {},
   "source": [
    "### Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26d0ad-42c8-4dcd-bfa3-e3339f0e8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader), [batch_time, losses, top1, top5], prefix=\"Test: \"\n",
    "    )\n",
    "\n",
    "    # Switch to evaluate mode.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Compute output.\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Measure accuracy and record loss.\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # Measure elapsed time.\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 10\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # print(\" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(top1=top1, top5=top5))\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45fe55-12a3-4ce5-8405-bfe1738488d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = validate(test_loader, model, criterion)\n",
    "print(f\"Accuracy of original PyTorch model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931283a-2ffd-42e4-a0b0-9ab4b874cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = validate(test_loader, quantized_model, criterion)\n",
    "print(f\"Accuracy of initialized INT8 model: {acc1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897799b2-0bad-406a-9502-b229398ac127",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a998608-0181-4da2-b065-0e100a19b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9f55a-8d9b-4fd6-aa05-e15e74888831",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(quantized_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761456fe-ba6f-43a6-8a33-4e0faf1490b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(inference_pytorch, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a659cf-07c5-48fc-8349-72bd19d0c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(inference_openvino, model=compiled_ov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a799c2-245b-44ef-aae5-5d12a7927a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit(inference_openvino, model=compiled_ov_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7273b1-8a7c-42e3-a74b-5d183a020178",
   "metadata": {},
   "source": [
    "# Benchmark Model Performance by Computing Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3207aee-5828-4181-abda-459c292bb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_benchmark_output(benchmark_output):\n",
    "    parsed_output = [line for line in benchmark_output if \"FPS\" in line]\n",
    "    print(*parsed_output, sep=\"\\n\")\n",
    "\n",
    "fp32_ir_path = 'ov_model.xml'\n",
    "int8_ir_path = 'ov_quantized_model.xml'\n",
    "\n",
    "openvino_device = 'CPU'\n",
    "\n",
    "print(\"Benchmark FP32 model (IR)\")\n",
    "benchmark_output = !benchmark_app -m $fp32_ir_path -d $openvino_device -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print(\"Benchmark INT8 model (IR)\")\n",
    "benchmark_output = !benchmark_app -m $int8_ir_path -d $openvino_device -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
